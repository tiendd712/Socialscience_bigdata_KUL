{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11bender/alumni-scraping/blob/main/alumni_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e4o5x1rcDZTN"
      },
      "source": [
        "# **Twitter scraper**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UQC8uAXpM1UO"
      },
      "outputs": [],
      "source": [
        "import os, random, sys, time \n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Tweet URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome('/driver/chromedriver')\n",
        "driver.get(\"https://www.twitter.com/login/\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Hashtag: #ARGFRA OR #WORLDCUPFINAL OR #argentinavsfrance\n",
        "- Time: until:2022-12-20 since:2022-12-16\n",
        "- Language: lang:en\n",
        "- Geocoding: geocode:54.2361,-2.5487,250mi (USA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-is:retweet #ARGFRA OR #WORLDCUPFINAL geocode:54.2361,-2.5487,250mi until:2022-12-19 since:2022-12-17 lang:en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEARCH QUERY\n",
        "url = \"https://twitter.com/search?q=-is%3Aretweet%20%23ARGFRA%20OR%20%23WORLDCUPFINAL%20until%3A2022-12-18%20since%3A2022-12-17%20lang%3Aen%20geocode%3A52.3555%2C-1.1743%2C250km&src=typed_query&f=top\"\n",
        "driver.get(url)\n",
        "# Initiate a list\n",
        "url_lst = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "stops = 0\n",
        "MAX_STOPS = 3\n",
        "\n",
        "while True:\n",
        "    page_source = driver.page_source\n",
        "    soup = BeautifulSoup(page_source, 'html.parser')\n",
        "        \n",
        "    urls = soup.find_all('div', {'class': \"css-1dbjc4n r-18u37iz r-1q142lx\"})\n",
        "    for url in urls:\n",
        "        try: \n",
        "            clean = url.find('a', {'class': 'css-4rbku5'})['href']\n",
        "            url_lst.append(clean)\n",
        "        except TypeError:\n",
        "            continue\n",
        "\n",
        "    time.sleep(2)\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(3)\n",
        "    \n",
        "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "    if new_height == current_height:\n",
        "        stops += 1\n",
        "        if stops == MAX_STOPS:\n",
        "            break\n",
        "        time.sleep(10)\n",
        "    else:\n",
        "        current_height = new_height\n",
        "        stops = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicates\n",
        "url_lst = list(set(url_lst))\n",
        "# Export to csv\n",
        "# Change name accordingly\n",
        "# england_url = pd.DataFrame(url_lst, columns=['url'])\n",
        "# england_url.to_csv('england_url.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scrape tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = []\n",
        "content_lst = []\n",
        "timestamp_lst = []\n",
        "emoji_lst = []\n",
        "reactions_lst = []\n",
        "is_retweeted = []\n",
        "is_quoted = []\n",
        "is_liked = []\n",
        "is_bookmarked = []\n",
        "\n",
        "twitter_url = \"https://www.twitter.com\"\n",
        "\n",
        "for i, url in enumerate(url_lst):\n",
        "  tweet_url = twitter_url + url\n",
        "  urls.append(tweet_url)\n",
        "  print((i+1)/len(url_lst))\n",
        "  driver.get(tweet_url)\n",
        "  time.sleep(5)\n",
        "  src = driver.page_source\n",
        "  soup = BeautifulSoup(src, 'html.parser')\n",
        "  \n",
        "  # scrape content\n",
        "  content = soup.find('div', {\"data-testid\":\"tweetText\"}).get_text()\n",
        "  content_lst.append(content)\n",
        "  # scrape timestamp\n",
        "  timestamp_lst.append(soup.find('time')['datetime'])\n",
        "  # scrape emojis\n",
        "  emojis = soup.find('div', {'class': \"css-901oao r-18jsvk2 r-37j5jr r-1inkyih r-16dba41 r-rjixqe r-bcqeeo r-bnwqim r-qvutc0\"})\n",
        "  emoji_lst.append(list(set([emo.get('src') for emo in emojis.find_all('img')])))\n",
        "  # scrape reactions\n",
        "  reaction = soup.find_all('span', {'class': \"css-901oao css-16my406 r-poiln3 r-1b43r93 r-b88u0q r-1cwl3u0 r-bcqeeo r-qvutc0\"})\n",
        "  reactions = [r.get_text() for r in reaction]\n",
        "  reactions_lst.append(reactions)\n",
        "  # has View or Retweet?\n",
        "  misc = soup.find_all('span', {'class': \"css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0\"})\n",
        "  stats = [m.get_text() for m in misc]\n",
        "  is_retweeted.append(1 if \"Retweet\" in stats else 0)\n",
        "  is_quoted.append(1 if \"Quote\" in stats else 0)\n",
        "  is_liked.append(1 if \"Likes\" in stats else 0)\n",
        "  is_bookmarked.append(1 if \"Bookmark\" in stats else 0)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.DataFrame(list(zip(urls, content_lst, emoji_lst, timestamp_lst, reactions_lst, is_retweeted, is_quoted, is_liked, is_bookmarked)), \n",
        "               columns =['url', 'tweet', 'emoji', 'timestamp', 'reactions', 'is_retweeted', 'is_quoted', 'is_liked', 'is_bookmarked']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.to_csv('xxx.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "alumni_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
